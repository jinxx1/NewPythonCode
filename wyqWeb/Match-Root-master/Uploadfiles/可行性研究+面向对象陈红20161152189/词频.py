def getTopkeyWordsTFIDF(stop_word_file_path,topK=100,content = ''):
    try:
        jieba.analyse.set_stop_words(stop_word_file_path)
        tags = jieba.analyse.extract_tags(content, topK, withWeight=True,allowPOS=('ns', 'n', 'vn', 'v'))
        for v, n in tags:
            print (v + '\t' + str((n )))
            top_word_dict_TFIDF[v] = n * 100
            #tfidf *100 作为词频
    except Exception as e:
        print(e)
    finally:
        pass


'''
算法	0.08462815202056018
图像	0.06854115641965353
数据	0.05283910802670873
文档	0.05101220109808328
使用	0.04392841012376796
函数	0.04240757682591333
查询	0.0403819432194448
匹配	0.037694924619685634
代码	0.036335922349209154
方法	0.03484516772501038
节点	0.03421192915540486
特征	0.03318907987532231
进行	0.03178994977740093
排序	0.029891585563684996
计算	0.029777524393560077
需要	0.029736538415988556
线程	0.029006587816953804
像素	0.028699044745897434
模型	0.027916255808773046
文件	0.027420392410540367
字段	0.026784762281347744
结果	0.026095752460980292
视差	0.024639602681519393
信息	0.024103853358438333
分片	0.02334856522790845
文章	0.021895636116826444
处理	0.02126962755753931
学习	0.021179099985705236
定义	0.020732334877947022
实现	0.020613687169542698
''
